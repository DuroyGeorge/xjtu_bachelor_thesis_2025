@misc{dao2023flashattention2fasterattentionbetter,
  title         = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author        = {Tri Dao},
  year          = {2023},
  eprint        = {2307.08691},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
}
@inproceedings{NEURIPS2022_b1efde53,
  author    = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {27730--27744},
  publisher = {Curran Associates, Inc.},
  title     = {Training language models to follow instructions with human feedback},
  volume    = {35},
  year      = {2022}
}
@misc{ouyang2022traininglanguagemodelsfollow,
  title         = {Training language models to follow instructions with human feedback},
  author        = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  year          = {2022},
  eprint        = {2203.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
}
@inproceedings{10.1145/3600006.3613165,
  author    = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  title     = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  year      = {2023},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3600006.3613165},
  abstract  = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  pages     = {611â€“626},
  numpages  = {16},
  location  = {Koblenz, Germany},
  series    = {SOSP '23}
}
@misc{zhao2025insightsdeepseekv3scalingchallenges,
  title         = {Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures},
  author        = {Chenggang Zhao and Chengqi Deng and Chong Ruan and Damai Dai and Huazuo Gao and Jiashi Li and Liyue Zhang and Panpan Huang and Shangyan Zhou and Shirong Ma and Wenfeng Liang and Ying He and Yuqing Wang and Yuxuan Liu and Y. X. Wei},
  year          = {2025},
  eprint        = {2505.09343},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC},
}
@inbook{Baldini2017,
  author    = {Baldini, Ioana
               and Castro, Paul
               and Chang, Kerry
               and Cheng, Perry
               and Fink, Stephen
               and Ishakian, Vatche
               and Mitchell, Nick
               and Muthusamy, Vinod
               and Rabbah, Rodric
               and Slominski, Aleksander
               and Suter, Philippe},
  editor    = {Chaudhary, Sanjay
               and Somani, Gaurav
               and Buyya, Rajkumar},
  title     = {Serverless Computing: Current Trends and Open Problems},
  booktitle = {Research Advances in Cloud Computing},
  year      = {2017},
  publisher = {Springer Singapore},
  address   = {Singapore},
  pages     = {1--20},
  abstract  = {Serverless computing has emerged as a new compelling paradigm for the deployment of applications and services. It represents an evolution of cloud programming models, abstractions, and platforms, and is a testament to the maturity and wide adoption of cloud technologies. In this chapter, we survey existing serverless platforms from industry, academia, and open-source projects, identify key characteristics and use cases, and describe technical challenges and open problems.},
  isbn      = {978-981-10-5026-8},
  doi       = {10.1007/978-981-10-5026-8_1},
}
@misc{jonas2019cloudprogrammingsimplifiedberkeley,
  title         = {Cloud Programming Simplified: A Berkeley View on Serverless Computing},
  author        = {Eric Jonas and Johann Schleier-Smith and Vikram Sreekanti and Chia-Che Tsai and Anurag Khandelwal and Qifan Pu and Vaishaal Shankar and Joao Carreira and Karl Krauth and Neeraja Yadwadkar and Joseph E. Gonzalez and Raluca Ada Popa and Ion Stoica and David A. Patterson},
  year          = {2019},
  eprint        = {1902.03383},
  archiveprefix = {arXiv},
  primaryclass  = {cs.OS},
}
@article{osti_10451467,
  place        = {Country unknown/Code not available},
  title        = {ReAct: Synergizing Reasoning and Acting in Language Models},
  abstractnote = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.},
  journal      = {International Conference on Learning Representations (ICLR)},
  author       = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan}
}
@inproceedings{NEURIPS2023_d842425e,
  author    = {Schick, Timo and Dwivedi-Yu, Jane and Dessi, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {68539--68551},
  publisher = {Curran Associates, Inc.},
  title     = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  volume    = {36},
  year      = {2023}
}
@inproceedings{298501,
  author    = {Bin Gao and Zhuomin He and Puru Sharma and Qingxuan Kang and Djordje Jevdjic and Junbo Deng and Xingkun Yang and Zhou Yu and Pengfei Zuo},
  title     = {{Cost-Efficient} Large Language Model Serving for Multi-turn Conversations with {CachedAttention}},
  booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
  year      = {2024},
  isbn      = {978-1-939133-41-0},
  address   = {Santa Clara, CA},
  pages     = {111--126},
  publisher = {USENIX Association},
  month     = jul
}
@inproceedings{NEURIPS2022_67d57c32,
  author    = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {16344--16359},
  publisher = {Curran Associates, Inc.},
  title     = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  volume    = {35},
  year      = {2022}
}
@inproceedings{NEURIPS2023_a452a7c6,
  author    = {Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {52342--52364},
  publisher = {Curran Associates, Inc.},
  title     = {Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time},
  volume    = {36},
  year      = {2023}
}
@misc{chen2023acceleratinglargelanguagemodel,
  title         = {Accelerating Large Language Model Decoding with Speculative Sampling},
  author        = {Charlie Chen and Sebastian Borgeaud and Geoffrey Irving and Jean-Baptiste Lespiau and Laurent Sifre and John Jumper},
  year          = {2023},
  eprint        = {2302.01318},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
}
@inproceedings{10.1145/3620666.3651335,
  author    = {Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and Shi, Chunan and Chen, Zhuoming and Arfeen, Daiyaan and Abhyankar, Reyna and Jia, Zhihao},
  title     = {SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification},
  year      = {2024},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3620666.3651335},
  abstract  = {This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree-based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8\texttimes{} for distributed LLM inference and by 2.6-3.5\texttimes{} for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages     = {932â€“949},
  numpages  = {18},
  keywords  = {large language model serving, speculative decoding, token tree verification},
  location  = {La Jolla, CA, USA},
  series    = {ASPLOS '24}
}
@misc{salmani2025llminferenceaccelerationefficient,
  title         = {LLM Inference Acceleration via Efficient Operation Fusion},
  author        = {Mahsa Salmani and Ilya Soloveychik},
  year          = {2025},
  eprint        = {2502.17728},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
}
@inproceedings{pmlr-v202-xiao23c,
  title     = {{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author    = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages     = {38087--38099},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  month     = {7},
  day       = {23--29},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf},
  abstract  = {Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56$\times$ speedup and 2$\times$ memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.}
}
@article{8594636,
  author   = {Malkov, Yu A. and Yashunin, D. A.},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs},
  year     = {2020},
  volume   = {42},
  number   = {4},
  pages    = {824-836},
  keywords = {Routing;Complexity theory;Search problems;Data models;Approximation algorithms;Biological system modeling;Brain modeling;Graph and tree search strategies;artificial intelligence;information search and retrieval;information storage and retrieval;information technology and systems;search process;graphs and networks;data structures;nearest neighbor search;big data;approximate search;similarity search},
  doi      = {10.1109/TPAMI.2018.2889473}
}
@misc{shoeybi2020megatronlmtrainingmultibillionparameter,
  title         = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author        = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  year          = {2020},
  eprint        = {1909.08053},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
}
@inproceedings{10.1145/3458817.3476209,
  author    = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  title     = {Efficient large-scale language model training on GPU clusters using megatron-LM},
  year      = {2021},
  isbn      = {9781450384421},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3458817.3476209},
  abstract  = {Large language models have led to state-of-the-art accuracies across several tasks. However, training these models efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52\% of theoretical peak).},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {58},
  numpages  = {15},
  location  = {St. Louis, Missouri},
  series    = {SC '21}
}
@misc{li2020pytorchdistributedexperiencesaccelerating,
  title         = {PyTorch Distributed: Experiences on Accelerating Data Parallel Training},
  author        = {Shen Li and Yanli Zhao and Rohan Varma and Omkar Salpekar and Pieter Noordhuis and Teng Li and Adam Paszke and Jeff Smith and Brian Vaughan and Pritam Damania and Soumith Chintala},
  year          = {2020},
  eprint        = {2006.15704},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC},
}
@misc{lim2025macragcompressslicescaleup,
  title         = {MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG},
  author        = {Woosang Lim and Zekun Li and Gyuwan Kim and Sungyoung Ji and HyeonJung Kim and Kyuri Choi and Jin Hyuk Lim and Kyungpyo Park and William Yang Wang},
  year          = {2025},
  eprint        = {2505.06569},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
}
@misc{li2025eacoragdistributedtieredllm,
  title         = {EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update},
  author        = {Jiaxing Li and Chi Xu and Lianchen Jia and Feng Wang and Cong Zhang and Jiangchuan Liu},
  year          = {2025},
  eprint        = {2410.20299},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC},
}
@misc{jin2024ragcacheefficientknowledgecaching,
  title         = {RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation},
  author        = {Chao Jin and Zili Zhang and Xuanlin Jiang and Fangyue Liu and Xin Liu and Xuanzhe Liu and Xin Jin},
  year          = {2024},
  eprint        = {2404.12457},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC},
}
@misc{stuhlmann2025efficientreproduciblebiomedicalquestion,
  title         = {Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation},
  author        = {Linus Stuhlmann and Michael Alexander Saxer and Jonathan FÃ¼rst},
  year          = {2025},
  eprint        = {2505.07917},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
}
@inproceedings{295471,
  author    = {Zili Zhang and Fangyue Liu and Gang Huang and Xuanzhe Liu and Xin Jin},
  title     = {Fast Vector Query Processing for Large Datasets Beyond {GPU} Memory with Reordered Pipelining},
  booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  year      = {2024},
  isbn      = {978-1-939133-39-7},
  address   = {Santa Clara, CA},
  pages     = {23--40},
  publisher = {USENIX Association},
  month     = apr
}
@misc{zhang2022bufferpoolawarequery,
  title         = {Buffer Pool Aware Query Scheduling via Deep Reinforcement Learning},
  author        = {Chi Zhang and Ryan Marcus and Anat Kleiman and Olga Papaemmanouil},
  year          = {2022},
  eprint        = {2007.10568},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DB},
}
@article{10.1109/TPAMI.2018.2889473,
  author     = {Malkov, Yu A. and Yashunin, D. A.},
  title      = {Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs},
  year       = {2020},
  issue_date = {April 2020},
  publisher  = {IEEE Computer Society},
  address    = {USA},
  volume     = {42},
  number     = {4},
  issn       = {0162-8828},
  doi        = {10.1109/TPAMI.2018.2889473},
  abstract   = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
  journal    = {IEEE Trans. Pattern Anal. Mach. Intell.},
  month      = apr,
  pages      = {824â€“836},
  numpages   = {13}
}
@misc{douze2025faisslibrary,
  title         = {The Faiss library},
  author        = {Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel MazarÃ© and Maria Lomeli and Lucas Hosseini and HervÃ© JÃ©gou},
  year          = {2025},
  eprint        = {2401.08281},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
}
@inproceedings{NEURIPS2019_09853c7f,
  author    = {Jayaram Subramanya, Suhas and Devvrit, Fnu and Simhadri, Harsha Vardhan and Krishnawamy, Ravishankar and Kadekodi, Rohan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node},
  volume    = {32},
  year      = {2019}
}
@article{10.1145/1327452.1327492,
  author     = {Dean, Jeffrey and Ghemawat, Sanjay},
  title      = {MapReduce: simplified data processing on large clusters},
  year       = {2008},
  issue_date = {January 2008},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {51},
  number     = {1},
  issn       = {0001-0782},
  doi        = {10.1145/1327452.1327492},
  abstract   = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
  journal    = {Commun. ACM},
  month      = jan,
  pages      = {107â€“113},
  numpages   = {7}
}
@inproceedings{10.5555/2228298.2228301,
  author    = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  title     = {Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing},
  year      = {2012},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
  booktitle = {Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation},
  pages     = {2},
  numpages  = {1},
  location  = {San Jose, CA},
  series    = {NSDI'12}
}
@article{10.14778/3137765.3137777,
  author     = {Carbone, Paris and Ewen, Stephan and F\'{o}ra, Gyula and Haridi, Seif and Richter, Stefan and Tzoumas, Kostas},
  title      = {State management in Apache FlinkÂ®: consistent stateful distributed stream processing},
  year       = {2017},
  issue_date = {August 2017},
  publisher  = {VLDB Endowment},
  volume     = {10},
  number     = {12},
  issn       = {2150-8097},
  doi        = {10.14778/3137765.3137777},
  abstract   = {Stream processors are emerging in industry as an apparatus that drives analytical but also mission critical services handling the core of persistent application logic. Thus, apart from scalability and low-latency, a rising system need is first-class support for application state together with strong consistency guarantees, and adaptivity to cluster reconfigurations, software patches and partial failures. Although prior systems research has addressed some of these specific problems, the practical challenge lies on how such guarantees can be materialized in a transparent, non-intrusive manner that relieves the user from unnecessary constraints. Such needs served as the main design principles of state management in Apache Flink, an open source, scalable stream processor.We present Flink's core pipelined, in-flight mechanism which guarantees the creation of lightweight, consistent, distributed snapshots of application state, progressively, without impacting continuous execution. Consistent snapshots cover all needs for system reconfiguration, fault tolerance and version management through coarse grained rollback recovery. Application state is declared explicitly to the system, allowing efficient partitioning and transparent commits to persistent storage. We further present Flink's backend implementations and mechanisms for high availability, external state queries and output commit. Finally, we demonstrate how these mechanisms behave in practice with metrics and large-deployment insights exhibiting the low performance trade-offs of our approach and the general benefits of exploiting asynchrony in continuous, yet sustainable system deployments.},
  journal    = {Proc. VLDB Endow.},
  month      = aug,
  pages      = {1718â€“1729},
  numpages   = {12}
}
@inproceedings{10.5555/3291168.3291210,
  author    = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
  title     = {Ray: a distributed framework for emerging AI applications},
  year      = {2018},
  isbn      = {9781931971478},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray--a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
  booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {561â€“577},
  numpages  = {17},
  location  = {Carlsbad, CA, USA},
  series    = {OSDI'18}
}
@inproceedings{rocklin2015dask,
  title     = {Dask: Parallel Computing with Task Scheduling},
  author    = {Matthew Rocklin},
  booktitle = {Proceedings of the 14th Python in Science Conference (SciPy 2015)},
  year      = {2015},
}
@book{10.5555/502981,
  author    = {Kennedy, Ken and Allen, John R.},
  title     = {Optimizing compilers for modern architectures: a dependence-based approach},
  year      = {2001},
  isbn      = {1558602860},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address   = {San Francisco, CA, USA},
  abstract  = {Modern computer architectures designed with high-performance microprocessors offer tremendous potential gains in performance over previous designs. Yet their very complexity makes it increasingly difficult to produce efficient code and to realize their full potential. This landmark text from two leaders in the field focuses on the pivotal role that compilers can play in addressing this critical issue. The basis for all the methods presented in this book is data dependence, a fundamental compiler analysis tool for optimizing programs on high-performance microprocessors and parallel architectures. It enables compiler designers to write compilers that automatically transform simple, sequential programs into forms that can exploit special features of these modern architectures. The text provides a broad introduction to data dependence, to the many transformation strategies it supports, and to its applications to important optimization problems such as parallelization, compiler memory hierarchy management, and instruction scheduling. The authors demonstrate the importance and wide applicability of dependence-based compiler optimizations and give the compiler writer the basics needed to understand and implement them. They also offer cookbook explanations for transforming applications by hand to computational scientists and engineers who are driven to obtain the best possible performance of their complex applications.}
}
@misc{gluon2017,
  author = {Matt Wood},
  title  = {Introducing Gluon: a new library for machine learning from AWS and Microsoft},
  year   = {2017},
}
@article{10.1145/2517327.2442559,
  author     = {Wozniak, Justin M. and Armstrong, Timothy G. and Wilde, Michael and Katz, Daniel S. and Lusk, Ewing and Foster, Ian T.},
  title      = {Swift/T: scalable data flow programming for many-task applications},
  year       = {2013},
  issue_date = {August 2013},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {48},
  number     = {8},
  issn       = {0362-1340},
  doi        = {10.1145/2517327.2442559},
  abstract   = {Swift/T, a novel programming language implementation for highly scalable data flow programs, is presented.},
  journal    = {SIGPLAN Not.},
  month      = feb,
  pages      = {309â€“310},
  numpages   = {2},
  keywords   = {turbine, swift, mpi, futures, exascale, dataflow, concurrency, adlb}
}
@inproceedings{10.1145/2442516.2442559,
  author    = {Wozniak, Justin M. and Armstrong, Timothy G. and Wilde, Michael and Katz, Daniel S. and Lusk, Ewing and Foster, Ian T.},
  title     = {Swift/T: scalable data flow programming for many-task applications},
  year      = {2013},
  isbn      = {9781450319225},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/2442516.2442559},
  abstract  = {Swift/T, a novel programming language implementation for highly scalable data flow programs, is presented.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages     = {309â€“310},
  numpages  = {2},
  keywords  = {turbine, swift, mpi, futures, exascale, dataflow, concurrency, adlb},
  location  = {Shenzhen, China},
  series    = {PPoPP '13}
}
@misc{lattner2020mlircompilerinfrastructureend,
  title         = {MLIR: A Compiler Infrastructure for the End of Moore's Law},
  author        = {Chris Lattner and Mehdi Amini and Uday Bondhugula and Albert Cohen and Andy Davis and Jacques Pienaar and River Riddle and Tatiana Shpeisman and Nicolas Vasilache and Oleksandr Zinenko},
  year          = {2020},
  eprint        = {2002.11054},
  archiveprefix = {arXiv},
  primaryclass  = {cs.PL},
}
@misc{ramoncortes2018autoparallelpythonmoduleautomatic,
  title         = {AutoParallel: A Python module for automatic parallelization and distributed execution of affine loop nests},
  author        = {Cristian Ramon-Cortes and Ramon Amela and Jorge Ejarque and Philippe Clauss and Rosa M. Badia},
  year          = {2018},
  eprint        = {1810.11268},
  archiveprefix = {arXiv},
  primaryclass  = {cs.PL},
}
@inproceedings{10.5555/256664.256760,
  author    = {Ban\^{a}tre, Michael},
  title     = {Hiding distribution in distributed systems},
  year      = {1991},
  isbn      = {0897913914},
  publisher = {IEEE Computer Society Press},
  address   = {Washington, DC, USA},
  booktitle = {Proceedings of the 13th International Conference on Software Engineering},
  pages     = {189â€“196},
  numpages  = {8},
  location  = {Austin, Texas, USA},
  series    = {ICSE '91}
}
@inproceedings{246288,
  author    = {Alexandru Agache and Marc Brooker and Alexandra Iordache and Anthony Liguori and Rolf Neugebauer and Phil Piwonka and Diana-Maria Popa},
  title     = {Firecracker: Lightweight Virtualization for Serverless Applications },
  booktitle = {17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)},
  year      = {2020},
  isbn      = {978-1-939133-13-7},
  address   = {Santa Clara, CA},
  pages     = {419--434},
  publisher = {USENIX Association},
  month     = feb
}
@inproceedings{216031,
  author    = {Edward Oakes and Leon Yang and Dennis Zhou and Kevin Houck and Tyler Harter and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},
  title     = {{SOCK}: Rapid Task Provisioning with {Serverless-Optimized} Containers},
  booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
  year      = {2018},
  isbn      = {978-1-931971-44-7},
  address   = {Boston, MA},
  pages     = {57--70},
  publisher = {USENIX Association},
  month     = jul
}
@inproceedings{215935,
  author    = {Istemi Ekin Akkus and Ruichuan Chen and Ivica Rimac and Manuel Stein and Klaus Satzke and Andre Beck and Paarijaat Aditya and Volker Hilt},
  title     = {{SAND}: Towards {High-Performance} Serverless Computing},
  booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
  year      = {2018},
  isbn      = {978-1-939133-01-4},
  address   = {Boston, MA},
  pages     = {923--935},
  publisher = {USENIX Association},
  month     = jul
}
@inproceedings{10.1145/3373376.3378512,
  author    = {Du, Dong and Yu, Tianyi and Xia, Yubin and Zang, Binyu and Yan, Guanglu and Qin, Chenggang and Wu, Qixuan and Chen, Haibo},
  title     = {Catalyzer: Sub-millisecond Startup for Serverless Computing with Initialization-less Booting},
  year      = {2020},
  isbn      = {9781450371025},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3373376.3378512},
  abstract  = {Serverless computing promises cost-efficiency and elasticity for high-productive software development. To achieve this, the serverless sandbox system must address two challenges: strong isolation between function instances, and low startup latency to ensure user experience. While strong isolation can be provided by virtualization-based sandboxes, the initialization of sandbox and application causes non-negligible startup overhead. Conventional sandbox systems fall short in low-latency startup due to their application-agnostic nature: they can only reduce the latency of sandbox initialization through hypervisor and guest kernel customization, which is inadequate and does not mitigate the majority of startup overhead.This paper proposes Catalyzer, a serverless sandbox system design providing both strong isolation and extremely fast function startup. Instead of booting from scratch, Catalyzer restores a virtualization-based function instance from a well-formed checkpoint image and thereby skips the initialization on the critical path (init-less). Catalyzer boosts the restore performance by on-demand recovering both user-level memory state and system state. We also propose a new OS primitive, sfork (sandbox fork), to further reduce the startup latency by directly reusing the state of a running sandbox instance. Fundamentally, Catalyzer removes the initialization cost by reusing state, which enables general optimizations for diverse serverless functions. The evaluation shows that Catalyzer reduces startup latency by orders of magnitude, achieves < 1ms latency in the best case, and significantly reduces the end-to-end latency for real-world workloads. Catalyzer has been adopted by Ant Financial, and we also present lessons learned from industrial development.},
  booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {467â€“481},
  numpages  = {15},
  keywords  = {startup latency, serverless computing, operating system, checkpoint and restore},
  location  = {Lausanne, Switzerland},
  series    = {ASPLOS '20}
}
@inproceedings{10.1145/3445814.3446757,
  author    = {Fuerst, Alexander and Sharma, Prateek},
  title     = {FaasCache: keeping serverless computing alive with greedy-dual caching},
  year      = {2021},
  isbn      = {9781450383172},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3445814.3446757},
  abstract  = {Functions as a Service (also called serverless computing) promises to revolutionize how applications use cloud resources. However, functions suffer from cold-start problems due to the overhead of initializing their code and data dependencies before they can start executing. Keeping functions alive and warm after they have finished execution can alleviate the cold-start overhead. Keep-alive policies must keep functions alive based on their resource and usage characteristics, which is challenging due to the diversity in FaaS workloads. Our insight is that keep-alive is analogous to caching. Our caching-inspired Greedy-Dual keep-alive policy can be effective in reducing the cold-start overhead by more than 3\texttimes{} compared to current approaches. Caching concepts such as reuse distances and hit-ratio curves can also be used for auto-scaled server resource provisioning, which can reduce the resource requirement of FaaS providers by 30\% for real-world dynamic workloads. We implement caching-based keep-alive and resource provisioning policies in our FaasCache system, which is based on OpenWhisk. We hope that our caching analogy opens the door to more principled and optimized keep-alive and resource provisioning techniques for future FaaS workloads and platforms.},
  booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {386â€“400},
  numpages  = {15},
  keywords  = {Caching, Cloud Computing, Functions as a Service, Serverless Computing},
  location  = {Virtual, USA},
  series    = {ASPLOS '21}
}
@article{myers1986nd,
  title     = {An O (ND) difference algorithm and its variations},
  author    = {Myers, Eugene W},
  journal   = {Algorithmica},
  volume    = {1},
  number    = {1},
  pages     = {251--266},
  year      = {1986},
  publisher = {Springer}
}